{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Baseline Features\n",
    "\n",
    "The baseline features include: `Timestamp`, `Open`, `High`, `Low`, `Close`, `Volume`.\n",
    "- Working with the full index dataset (7.0M entries) can lead to excessive training times.\n",
    "- We'll restrict our dataset to the most recent `100,000` data points, which are likely more relevant for future predictions.\n",
    "- Note that a `training period` of only two months may not capture all relevant trends, despite containing substantial data."
   ],
   "id": "385d23aed20b454f"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PROCESSED_PATH = \"data/processed/\"\n",
    "PROCESSED_NAME = \"btcusd_1-min_data_processed.csv\"\n",
    "PROCESSED_FILE = os.path.join(PROCESSED_PATH, PROCESSED_NAME)\n",
    "\n",
    "df = pd.read_csv(PROCESSED_FILE)\n",
    "df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"s\", utc=True)\n",
    "df.set_index(\"Timestamp\", inplace=True)\n",
    "df.drop(columns=[\"datetime\"], inplace=True)\n",
    "df.info()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Splitting Strategy\n",
    "\n",
    "For our time series, we'll split the dataset into three distinct segments:\n",
    "\n",
    "1. **Training Set (60%)**: The earliest portion of our chronological data used to train the model and establish patterns.\n",
    "\n",
    "2. **Validation Set (20%)**: The middle segment used to tune hyperparameters and prevent overfitting during the development phase.\n",
    "\n",
    "3. **Test Set (20%)**: The most recent data, kept completely separate until final evaluation to simulate real-world performance.\n",
    "\n",
    "This chronological splitting approach is crucial for financial time series data to prevent data leakage and maintain the temporal nature of the information."
   ],
   "id": "3e3863dfe6766756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Chronological split with 60/20/20\n",
    "train_size = int(0.6 * len(df))\n",
    "valid_size = int(0.2 * len(df))\n",
    "\n",
    "# Split in chronological order\n",
    "train_data = df.iloc[:train_size]\n",
    "valid_data = df.iloc[train_size : train_size + valid_size]\n",
    "test_data = df.iloc[train_size + valid_size :]\n",
    "\n",
    "print(f\"Training set: {len(train_data)} entries\")\n",
    "print(f\"Validation set: {len(valid_data)} entries\")\n",
    "print(f\"Test set: {len(test_data)} entries\")\n",
    "\n",
    "# Check time ranges\n",
    "print(f\"Training set: {train_data.index.min()} to {train_data.index.max()}\")\n",
    "print(f\"Validation set: {valid_data.index.min()} to {valid_data.index.max()}\")\n",
    "print(f\"Test set: {test_data.index.min()} to {test_data.index.max()}\")\n",
    "\n",
    "# Saving the split data records\n",
    "train_data.to_csv(os.path.join(PROCESSED_PATH, \"train_data.csv\"))\n",
    "valid_data.to_csv(os.path.join(PROCESSED_PATH, \"validation_data.csv\"))\n",
    "test_data.to_csv(os.path.join(PROCESSED_PATH, \"test_data.csv\"))"
   ],
   "id": "4da7b5fbd668148a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TARGET VARIABLE\n",
    "\n",
    "- We define our prediction variable `signal` using the `Close` price relative to the Bull Market Support Band (the lower of 20-week SMA and 21-week EMA).\n",
    "- If closing price falls below the Bull Market Support Band, it signals a bear market, otherwise a bull market.\n",
    "- The trading strategy assigns signal value = 1 (buy) in bull markets and signal value = 0 (sell) in bear markets.\n",
    "- The window values for both moving averages are configurable parameters, both of which are arbitrary, and can affect the results, ideally an optimisation study needs to be carried out to find optimum values.\n"
   ],
   "id": "142b511d02b42ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_bull_bear_signals(price_data, verbose=True):\n",
    "    # Calculate 20-week SMA for minute-level data\n",
    "    # 20 weeks = 20 weeks * 7 days * 24 hours * 60 minutes = 201,600 minutes\n",
    "    price_data[\"SMA_20W\"] = (\n",
    "        price_data[\"Close\"].rolling(window=201600, min_periods=1, center=False).mean()\n",
    "    )\n",
    "\n",
    "    # Calculate 21-week EMA for minute-level data\n",
    "    # 21 weeks = 21 weeks * 7 days * 24 hours * 60 minutes = 211,680 minutes\n",
    "    price_data[\"EMA_21W\"] = (\n",
    "        price_data[\"Close\"].ewm(span=211680, min_periods=1, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # Create Bull Market Support Band (the lower of the two indicators)\n",
    "    price_data[\"Bull_Support_Band\"] = price_data[[\"SMA_20W\", \"EMA_21W\"]].min(axis=1)\n",
    "\n",
    "    # Create signals: 1 (buy) when Close is above Bull Market Support Band, 0 (sell) otherwise\n",
    "    price_data[\"signal\"] = np.where(\n",
    "        price_data[\"Close\"] > price_data[\"Bull_Support_Band\"], 1.0, 0.0\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        display(price_data[\"signal\"].value_counts())\n",
    "\n",
    "    return price_data\n",
    "\n",
    "\n",
    "train_data_with_signals = train_data.copy()\n",
    "valid_data_with_signals = valid_data.copy()\n",
    "\n",
    "generate_bull_bear_signals(train_data_with_signals, verbose=True)\n",
    "generate_bull_bear_signals(valid_data_with_signals, verbose=False)"
   ],
   "id": "e9a9c4565a8727b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_target_correlation(df, target=\"demand\", figsize=(12, 1), return_corr=False):\n",
    "    # Calculate correlation matrix\n",
    "    corr = df.corr()[target].drop(target, errors=\"ignore\")\n",
    "\n",
    "    if return_corr:\n",
    "        return corr\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        corr.to_frame().T,\n",
    "        annot=True,\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "        vmin=-0.3,\n",
    "        vmax=0.3,\n",
    "        cbar=False,\n",
    "        linewidths=1,\n",
    "    )\n",
    "    plt.title(f\"Feature Correlation with {target}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt.gcf()"
   ],
   "id": "e29c8284a2bed077",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_target_correlation(train_data_with_signals, \"signal\", figsize=(7, 0.5))",
   "id": "84eb4f9c7b5bdff3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
